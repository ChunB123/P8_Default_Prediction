{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50182cf4-66b2-48f1-8f20-60c857de1464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy, cudf\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt, gc, os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c668d-6561-4cf0-bf05-a15ae35186e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ea7d2-2cbe-4f5b-a31d-a1a5ba9bda59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim=95, feat_dim=188, num_heads=4, ff_dim=254, rate=0.35):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(feat_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbf3004-c2a7-4723-a508-b336f3cb22d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks = 2\n",
    "\n",
    "def build_model():\n",
    "    \n",
    "    inp = layers.Input(shape=(13,188))\n",
    "    \n",
    "#     embeddings = []\n",
    "#     for k in range(11):\n",
    "#         emb = layers.Embedding(10,4)\n",
    "#         embeddings.append( emb(inp[:,:,k]) )\n",
    "#     x = layers.Concatenate()([inp[:,:,11:]]+embeddings)\n",
    "#     x = layers.Dense(feat_dim)(x)\n",
    "    \n",
    "    x = inp\n",
    "    for k in range(num_blocks):\n",
    "        # x_old = x\n",
    "        transformer_block = TransformerBlock(embed_dim, feat_dim, num_heads, ff_dim, dropout_rate)\n",
    "        x = transformer_block(x)\n",
    "        # x = 0.9*x + 0.1*x_old\n",
    "    \n",
    "    x = layers.Dense(84, activation=\"relu\")(x[:,-1,:])\n",
    "    x = layers.Dense(42, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inp, outputs=outputs)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(loss=loss, optimizer = opt)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be6832-71bc-47e8-aabd-daa08f674caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrd(epoch):\n",
    "    if epoch == 1:\n",
    "        return 1e-3\n",
    "    elif epoch == 2:\n",
    "        return 1e-3\n",
    "    elif epoch == 3:\n",
    "        return 1e-4\n",
    "    elif epoch == 4:\n",
    "        return 1e-4\n",
    "    elif epoch == 5:\n",
    "        return 1e-5:\n",
    "    elif epoch == 6:\n",
    "        return 1e-6\n",
    "\n",
    "LR = tf.keras.callbacks.LearningRateScheduler(lrd, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2837171c-3dc5-4839-9237-93ef6d0e0624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric_mod(y_true, y_pred):\n",
    "\n",
    "    labels     = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels     = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights    = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels         = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels         = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight         = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random  = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos      = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz        = cum_pos_found / total_pos\n",
    "        gini[i]        = np.sum((lorentz - weight_random) * weight)\n",
    "\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b252674-ddb1-4217-96d8-010a35eb2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = np.array([])\n",
    "oof = np.array([])\n",
    "\n",
    "for fold in range(5):\n",
    "\n",
    "    valid_idx = [2*fold+1, 2*fold+2]\n",
    "    train_idx = [x for x in [1,2,3,4,5,6] if x not in valid_idx]\n",
    "\n",
    "    print(f'### Fold {fold+1} with valid files', valid_idx)\n",
    "\n",
    "    X_train = []; y_train = []\n",
    "    for k in train_idx:\n",
    "        X_train.append( np.load(f'data_{k}.npy'))\n",
    "        y_train.append( pd.read_parquet(f'targets_{k}.pqt') )\n",
    "    X_train = np.concatenate(X_train,axis=0)\n",
    "    y_train = pd.concat(y_train).target.values\n",
    "    print('### Training data shapes', X_train.shape, y_train.shape)\n",
    "    \n",
    "    X_valid = []; y_valid = []\n",
    "    for k in valid_idx:\n",
    "        X_valid.append( np.load(f'data_{k}.npy'))\n",
    "        y_valid.append( pd.read_parquet(f'targets_{k}.pqt') )\n",
    "    X_valid = np.concatenate(X_valid,axis=0)\n",
    "    y_valid = pd.concat(y_valid).target.values\n",
    "    \n",
    "    print('### Validation data shapes', X_valid.shape, y_valid.shape)\n",
    "    print('#'*25)\n",
    "\n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "    h = model.fit(X_train,y_train, \n",
    "                  validation_data = (X_valid,y_valid),\n",
    "                  batch_size=512, epochs=EPOCHS, verbose=VERBOSE,\n",
    "                  callbacks = [LR])\n",
    "    if not os.path.exists(modelpath): os.makedirs(modelpath)\n",
    "    model.save_weights(f'{modelpath}transformer_fold_{fold+1}.h5')\n",
    "\n",
    "    print('Inferring validation data...')\n",
    "    p = model.predict(X_valid, batch_size=512, verbose=VERBOSE).flatten()\n",
    "\n",
    "    print()\n",
    "    print(f'Fold {fold+1} CV=', amex_metric_mod(y_valid, p) )\n",
    "    print()\n",
    "    true = np.concatenate([true, y_valid])\n",
    "    oof = np.concatenate([oof, p])\n",
    "\n",
    "print(f'Overall CV =', amex_metric_mod(true, oof) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd9e13-522c-41d8-a3db-7515ce9bdeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = build_model()\n",
    "\n",
    "start = 0; end = 0\n",
    "sub = cudf.read_csv('data/sample_submission.csv')\n",
    "\n",
    "sub['hash'] = sub['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "test_hash_index = cupy.load(f'data/TestData/test_hashes_data.npy')\n",
    "sub = sub.set_index('hash').loc[test_hash_index].reset_index(drop=True)\n",
    "\n",
    "for k in range(20):\n",
    "    print(f'Inferring Test_File_{k+1}')\n",
    "    X_test = np.load(f'data/TestData/test_data_{k+1}.npy')\n",
    "\n",
    "    end = start + X_test.shape[0]\n",
    "\n",
    "    model.load_weights(f'{modelpath}transformer_fold_1.h5')\n",
    "    p = model.predict(X_test, batch_size=512, verbose=0).flatten() \n",
    "    for j in range(1,5):\n",
    "        model.load_weights(f'{modelpath}transformer_fold_{j+1}.h5')\n",
    "        p += model.predict(X_test, batch_size=512, verbose=0).flatten()\n",
    "    p /= 5.0\n",
    "\n",
    "    sub.loc[start:end-1,'prediction'] = p\n",
    "    start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c5a2e7-e041-44a0-b0fb-331edf8d9c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(f'submission_8.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
